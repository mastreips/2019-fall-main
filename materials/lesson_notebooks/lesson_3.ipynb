{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side Notes for Lesson 3\n",
    "\n",
    "### 1. Simple word embeddings with nltk and gensim\n",
    "\n",
    "Following: https://github.com/nltk/nltk/blob/develop/nltk/test/gensim.doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.data import find\n",
    "\n",
    "import gensim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a couple of helper functions for cosine similarities: one deriving similarity between two words in the context of a model, the other for two vectors directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim_words(vec_model, a, b):\n",
    "    \"\"\"\n",
    "    arguments: word a, word b\n",
    "    return: cosine similarity between accociated model vectors with a and b\n",
    "    \"\"\"\n",
    "    \n",
    "    vec_a = vec_model[a]\n",
    "    vec_b = vec_model[b]\n",
    "    \n",
    "    return np.dot(vec_a, vec_b)/np.sqrt(np.dot(vec_a, vec_a))/np.sqrt(np.dot(vec_b, vec_b))\n",
    "\n",
    "def cossim_vecs(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    arguments: word a, word b\n",
    "    return: cosine similarity between accociated model vectors with a and b\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.dot(vec_a, vec_b)/np.sqrt(np.dot(vec_a, vec_a))/np.sqrt(np.dot(vec_b, vec_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download NLTK's sample word2vec embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /Users/joachim/nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the embeddings into a gensim model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43981"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the **embedding dimension**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model['university'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with cosine similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5080747"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossim_words(model, 'university', 'school')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now... let's try to 'construct' the embedding vector for queen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_queen = model['king'] + (model['woman'] - model['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71181935"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossim_vecs(model['queen'], model_queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare 'queen' to other similar words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65109587"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossim_words(model, 'queen', 'king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Our reconstructed vector seems to be decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple BOW Classification using Word Embeddings in Keras\n",
    "\n",
    "This should roughly implement the model on slides 41 in a toy setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43981"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we know the number of words that have an embedding. Let's build the embedding matrix from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = len(model['university'])      # we know... it's 300\n",
    "\n",
    "# initialize embedding matrix and word-to-id map:\n",
    "embedding_matrix = np.zeros((len(model.vocab.keys()) + 1, EMBEDDING_DIM))       \n",
    "vocab_dict = {}\n",
    "\n",
    "# build the embedding matrix and the word-to-id map:\n",
    "for i, word in enumerate(model.vocab.keys()):\n",
    "    embedding_vector = model[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        vocab_dict[word] = i\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43982, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct? Looks right.\n",
    "\n",
    "Let's build the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0908 21:51:02.776325 4572186048 deprecation.py:506] From /anaconda3/envs/tf1_14/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 5  # Keras' embedding layer expects a specific input length. Padding is often needed here.\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],          ## note: depreciated!\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the 'trainable=False' flag. **Q: What would happen if we had set it to true?**\n",
    "\n",
    "Using this embedding layer, let's use **Keras' Functional API** as opposed to the sequential model we looked at last week. The format is a bit different, but not very much.\n",
    "\n",
    "Start with defining the input, then add the layers sequentially acting on the previous layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0908 21:51:03.280385 4572186048 deprecation.py:506] From /anaconda3/envs/tf1_14/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Input layer:\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# apply the Embedding layer to the input layer\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# add all of the \n",
    "sum_embeddings =  K.sum(embedded_sequences, axis=1)       # for future reference: Lamba layers are good here...\n",
    "\n",
    "hidden = Dense(100, activation='relu')(sum_embeddings)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are the dimensions of the layers?**\n",
    "\n",
    "Next: we build the model, defining input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether our dimension discussion was correct. Print a model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5)]               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 5, 300)            13194600  \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowO [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 13,224,801\n",
      "Trainable params: 30,201\n",
      "Non-trainable params: 13,194,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bow_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like last week... let's compile the model. I.e, define optimizer, loss function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0908 21:51:03.352510 4572186048 deprecation.py:323] From /anaconda3/envs/tf1_14/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "bow_model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there... let's create some fake training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = ['this is really absolutely great', 'this is really absolutely terrible']\n",
    "train_labels = [[1], [0]]\n",
    "\n",
    "test_sentences = ['never seen anything this stupid', 'never seen anything this fantastic']\n",
    "test_labels = [[0], [1]]\n",
    "\n",
    "\n",
    "def sents_to_ids(sentences):\n",
    "    \"\"\"\n",
    "    converting a list of strings to a list of lists of word ids\n",
    "    \"\"\"\n",
    "    text_ids = []\n",
    "    for sentence in sentences:\n",
    "        example = []\n",
    "        for word in sentence.split(' '):\n",
    "            example.append(vocab_dict[word])\n",
    "        text_ids.append(example)\n",
    "\n",
    "    return  text_ids   \n",
    "\n",
    "\n",
    "train_input = np.array(sents_to_ids(train_sentences))\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_input = np.array(sents_to_ids(test_sentences))\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Before we start... should this come out ok-ish?**\n",
    "\n",
    "Next: let's get the start predictions. Should be random-ish. Are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4580708]\n",
      " [0.430571 ]]\n",
      "[[0.42396268]\n",
      " [0.4473657 ]]\n"
     ]
    }
   ],
   "source": [
    "print(bow_model.predict(train_input))\n",
    "print(bow_model.predict(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup.\n",
    "\n",
    "Finally... let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 samples, validate on 2 samples\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - 0s 67ms/sample - loss: 0.6719 - val_loss: 0.6643\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 1ms/sample - loss: 0.6464 - val_loss: 0.6545\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 1ms/sample - loss: 0.6306 - val_loss: 0.6451\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.6152 - val_loss: 0.6368\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.6002 - val_loss: 0.6287\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 1ms/sample - loss: 0.5854 - val_loss: 0.6208\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 2ms/sample - loss: 0.5707 - val_loss: 0.6131\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.5560 - val_loss: 0.6052\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 4ms/sample - loss: 0.5416 - val_loss: 0.5965\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 5ms/sample - loss: 0.5272 - val_loss: 0.5878\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 5ms/sample - loss: 0.5129 - val_loss: 0.5789\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 4ms/sample - loss: 0.4986 - val_loss: 0.5698\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.4842 - val_loss: 0.5605\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.4701 - val_loss: 0.5515\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 4ms/sample - loss: 0.4558 - val_loss: 0.5425\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.4421 - val_loss: 0.5333\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 3ms/sample - loss: 0.4282 - val_loss: 0.5242\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 5ms/sample - loss: 0.4147 - val_loss: 0.5153\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 7ms/sample - loss: 0.4015 - val_loss: 0.5069\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 6ms/sample - loss: 0.3881 - val_loss: 0.4989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a20d2e208>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_model.fit(train_input, train_labels, validation_data=(test_input, test_labels), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look's good!\n",
    "\n",
    "What are train & test predictions now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6886179 ],\n",
       "       [0.31423044]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_model.predict(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yey! But we obviously cheated here with the choice of sentences. Nevertheless, the idea should be clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1_14] *",
   "language": "python",
   "name": "conda-env-tf1_14-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
