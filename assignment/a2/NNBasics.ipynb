{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Basic s\n",
    "\n",
    "### Brief Review of Machine Learning\n",
    "\n",
    "In supervised learning, parametric models are those where the model is a function of a fixed form with a number of unknown _parameters_.  Together with a loss function and a training set, an optimizer can select parameters to minimize the loss with respect to the training set.  Common optimizers include stochastic gradient descent.  It tweaks the parameters slightly to move the loss \"downhill\" due to a small batch of examples from the training set.\n",
    "\n",
    "## Part A:  Linear & Logistic Regression\n",
    "\n",
    "You've likely seen linear regression before.  In linear regression, we fit a line (technically, hyperplane) that predicts a target variable, $y$, based on some features $x$.  The form of this model is affine (even if we call it \"linear\"):  \n",
    "\n",
    "$$y_{hat} = xW + b$$\n",
    "\n",
    "where $W$ and $b$ are weights and an offset, respectively, and are the parameters of this parametric model.  The loss function that the optimizer uses to fit these parameters is the squared error ($||\\cdots||_2$) between the prediction and the ground truth in the training set.\n",
    "\n",
    "You've also likely seen logistic regression, which is tightly related to linear regression.  Logistic regression also fits a line - this time separating the positive and negative examples of a binary classifier.  The form of this model is similar: \n",
    "\n",
    "$$y_{hat} = \\sigma(xW + b)$$\n",
    "\n",
    "where again $W$ and $b$ are the parameters of this model, and $\\sigma$ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) which maps un-normalized scores (\"logits\") to values $\\hat{y} \\in [0,1]$ that represent probabilities. The loss function that the optimizer uses to fit these parameters is the [cross entropy](../information_theory.ipynb) between the prediction and the ground truth in the training set.\n",
    "\n",
    "This pattern of an affine transform, $xW + b$, occurs over and over in machine learning.\n",
    "\n",
    "**We'll use logistic regression as our running example for the rest of this part.**\n",
    "\n",
    "\n",
    "### Short Answer Questions\n",
    "\n",
    "Imagine you want to implement logistic regression:\n",
    "\n",
    "* `z = xW + b`\n",
    "* `y_hat = sigmoid(z)`\n",
    "\n",
    "Where:\n",
    "1.  `x` is a 10-dimensional feature vector\n",
    "2.  `W` is the weight vector\n",
    "3.  `b` is the bias term\n",
    "\n",
    "What are the dimensions of `W` and `b`?  Recall that in logistic regression, `z` is just a scalar (commonly referred to as the \"logit\").\n",
    "\n",
    "Sketch a picture of the whole equation using rectangles to illustrate the dimensions of `x`, `W`, and `b`.  See examples below for inspiration (though please label each dimension).  We don't ask you to submit this, but make sure you can do it!  It's the \"print\" debugging statement of neural networks!  It's also useful for reading papers... if you can't draw the shapes of all the tensors, you don't (yet) know what's going on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Batching\n",
    "\n",
    "Let's say we want to perform inference using your model (parameters `W` and `b`) above on 10 examples intsead of just 1. On modern hardware (especially GPUs), we can do this efficiently by *batching*.\n",
    "\n",
    "To do this, we stack up the feature vectors in x like in the diagram below.  Note that changing the number of examples you run on (i.e. your batch size) *does not* affect the number of parameters in your model.  You're just running the same thing in parallel (instead of running the above one feature vector at a time at a time).\n",
    "\n",
    "![](batchaffine.png)\n",
    "\n",
    "The red (# features) and blue (batch size) lines represent dimensions that are the same.\n",
    "\n",
    "### Short Answer Questions\n",
    "\n",
    "If we have 10 features and running the model in parallel with 30 examples, what are the dimensions of:\n",
    "\n",
    "1. `W` ?\n",
    "2. `b` ?\n",
    "3. `x` ?\n",
    "4. `z` ?\n",
    "\n",
    "_Hint:_ remember that your model parameters stay fixed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Logistic Regression - NumPy Implementation\n",
    "\n",
    "In this section, we'll implement logistic regression by hand and compute a few values to make sure we understand what's going on!\n",
    "\n",
    "Let's say your model has the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.array([45,6,3,25,-1])\n",
    "b = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the model on the following three examples:\n",
    "\n",
    "* [1, 2, 3, 4, 5]\n",
    "* [0, 0, 0, 0, 5]\n",
    "* [-3, -4, -12, -1, 1]\n",
    "\n",
    "Construct the x matrix **such that you compute the answer all in one big batch** and compute the probability of the positive class for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   2   3   4   5]\n",
      " [  0   0   0   0   5]\n",
      " [ -3  -4 -12  -1   1]]\n",
      "z_2:  [1.  0.5 0. ]\n",
      "z_3:  [1.  0.5 0. ]\n"
     ]
    }
   ],
   "source": [
    "# Import sigmoid.\n",
    "from scipy.special import expit as sigmoid\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "x = np.array([[1,2,3,4,5], [0,0,0,0,5], [-3,-4,-12,-1,1]])\n",
    "print(x)\n",
    "# z_1 = sigmoid(x*W+b)\n",
    "# print(\"z_1: \", z_1)\n",
    "\n",
    "z_2 = sigmoid(np.matmul(x,W)+b)\n",
    "print(\"z_2: \", z_2)\n",
    "\n",
    "z_3 = sigmoid(np.dot(x,W)+b)  # Check \n",
    "print(\"z_3: \", z_3)\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n",
      "0.6931471805599453\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "p=1\n",
    "q=0.5\n",
    "ce = -np.dot(p, np.log(q))  # from lesson 2\n",
    "print(ce)\n",
    "\n",
    "print(-np.mean(p*np.log(q) + (1-p)*np.log(1-q))) #checkb\n",
    "\n",
    "p=1\n",
    "q=0.5\n",
    "ce2 = -np.dot(p, np.log2(q))  # from lesson 2\n",
    "print(ce2)\n",
    "print(-np.mean(p*np.log2(q) + (1-p)*np.log2(1-q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p=np.array([0,0,0,0,5])\n",
    "# q=0.5\n",
    "# ce = -np.dot(p, np.log(q))  # from lesson 2\n",
    "# print(ce)\n",
    "\n",
    "# print(-np.mean(p*np.log(q) + (1-p)*np.log(1-q))) #checkb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Answer Questions\n",
    "\n",
    "1. What is the probability of the positive class for the second (middle) example?\n",
    "2. What is the cross-entropy loss of the second example if its label is positive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: NumPy Feed Forward Neural Network\n",
    "\n",
    "Let's do the same procedure for a simple feed-forward neural network.\n",
    "\n",
    "Imagine you have a 3 layer network.  Each hidden layer is size 10.  Just like before, you've already trained your model and you just want to run it forward.  For this exercise, let's say that each weight matrix is np.ones(...) and each bias term is [-1, -2, -3, ..., -n] if the bias term is $n$ long.  Compute the probability of the positive class for the three examples above, again in a single batch.\n",
    "\n",
    "**Hint:  Draw the shapes of the matrices at each layer out on a piece of paper!  Include it with any questions you post to Piazza.**\n",
    "\n",
    "Assume your model uses a sigmoid as the nonlinearity for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (3, 5)\n",
      "W_in (5, 10)\n",
      "h1:  (3, 10)\n",
      "z1:  (3, 10)\n",
      "b1:  (10,)\n",
      "b:  [ -1  -2  -3  -4  -5  -6  -7  -8  -9 -10]\n",
      "z1:  (3, 10)\n",
      "W_h (10, 10)\n",
      "h2:  (3, 10)\n",
      "z2:  (3, 10)\n",
      "b2:  (10,)\n",
      "b:  [ -1  -2  -3  -4  -5  -6  -7  -8  -9 -10]\n",
      "z2:  (3, 10)\n",
      "W_h (10, 10)\n",
      "h3:  (3, 10)\n",
      "z3:  (3, 10)\n",
      "b3:  (10,)\n",
      "b:  [ -1  -2  -3  -4  -5  -6  -7  -8  -9 -10]\n",
      "z3:  (3, 10)\n",
      "W_out (10, 1)\n",
      "h3:  (3, 10)\n",
      "final:  (3, 1)\n",
      "b4:  -1\n",
      "final_z dims:  (3, 1)\n",
      "final_z: \n",
      " [[0.99934417]\n",
      " [0.92744813]\n",
      " [0.41696628]]\n",
      "probability of 3rd example:  [0.41696628]\n",
      "cross-entropy loss:  0.7783487725678238\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "x = np.array([[1,2,3,4,5], [0,0,0,0,5], [-3,-4,-12,-1,1]])\n",
    "y = 0\n",
    "b = np.arange(-1, -11, -1)\n",
    "outputSize = 1\n",
    "hiddenSize = 10\n",
    "inputSize = x.shape[1]\n",
    "# print(inputSize)\n",
    "# print(hiddenSize)\n",
    "W_in = np.ones((inputSize, hiddenSize))\n",
    "W_h = np.ones((hiddenSize, hiddenSize))\n",
    "W_out = np.ones((hiddenSize, outputSize))\n",
    "\n",
    "h1 = np.dot(x, W_in) + b\n",
    "z1 = sigmoid(h1)\n",
    "\n",
    "print('x: ', x.shape)\n",
    "print('W_in', W_in.shape)\n",
    "print('h1: ', h1.shape)\n",
    "print('z1: ', z1.shape)\n",
    "print('b1: ', b.shape)\n",
    "print('b: ', b)\n",
    "\n",
    "h2 = np.dot(z1, W_h) + b\n",
    "z2 = sigmoid(h2)\n",
    "\n",
    "print('z1: ', z1.shape)\n",
    "print('W_h', W_h.shape)\n",
    "print('h2: ', h2.shape)\n",
    "print('z2: ', z2.shape)\n",
    "print('b2: ', b.shape)\n",
    "print('b: ', b)\n",
    "\n",
    "h3 = np.dot(z2, W_h) + b\n",
    "z3 = sigmoid(h3)\n",
    "\n",
    "print('z2: ', z2.shape)\n",
    "print('W_h', W_h.shape)\n",
    "print('h3: ', h3.shape)\n",
    "print('z3: ', z3.shape)\n",
    "print('b3: ', b.shape)\n",
    "print('b: ', b)\n",
    "\n",
    "output = np.dot(z3, W_out) + b[0]\n",
    "final_z = sigmoid(output)\n",
    "\n",
    "print('z3: ', z3.shape)\n",
    "print('W_out', W_out.shape)\n",
    "print('h3: ', h3.shape)\n",
    "print('final: ', final_z.shape)\n",
    "print('b4: ', b[0])\n",
    "\n",
    "print('final_z dims: ', final_z.shape)\n",
    "print('final_z: \\n', final_z)\n",
    "\n",
    "p = 0\n",
    "q = final_z\n",
    "\n",
    "print('probability of 3rd example: ', final_z[2])\n",
    "# ce = -np.dot(p, np.log2(q[2])) \n",
    "# print(ce)\n",
    "\n",
    "print('cross-entropy loss: ', -np.mean(p*np.log2(q[2]) + (1-p)*np.log2(1-q[2])))\n",
    "\n",
    "    \n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      " [[0.99934417]\n",
      " [0.92744813]\n",
      " [0.41696628]]\n",
      "Actual Output: \n",
      " 0\n",
      "Cross-Entropy: \n",
      " [[-0.]\n",
      " [-0.]\n",
      " [-0.]]\n",
      "[0.41696628]\n",
      "[0.]\n",
      "0.7783487725678238\n"
     ]
    }
   ],
   "source": [
    "#### Alternative as a check.\n",
    "\n",
    "from scipy.special import expit as sigmoid\n",
    "x = np.array([[1,2,3,4,5], [0,0,0,0,5], [-3,-4,-12,-1,1]])\n",
    "\n",
    "# y = np.arrary([0],[1])\n",
    "y = 0\n",
    "b = np.arange(-1, -11, -1)\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        #parameters\n",
    "        self.inputSize = 5\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 10\n",
    "\n",
    "        #weights\n",
    "        self.W1 = np.ones((self.inputSize, self.hiddenSize))\n",
    "        self.W2 = np.ones((self.hiddenSize, self.hiddenSize))\n",
    "        self.W3 = np.ones((self.hiddenSize, self.hiddenSize))\n",
    "        self.W4 = np.ones((self.hiddenSize, self.outputSize))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #forward propagation through our network\n",
    "        self.z = np.dot(X, self.W1) # dot product of X (input) and first set of 3x2 weights\n",
    "        self.z2 = self.sigmoid(self.z + b) # activation function\n",
    "        self.z3 = np.dot(self.z2, self.W2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "        self.z4 = self.sigmoid(self.z3 + b)\n",
    "        self.z5 = np.dot(self.z4, self.W3)\n",
    "        self.z6 = self.sigmoid(self.z5 + b)\n",
    "        self.z7 = np.dot(self.z6, self.W4)\n",
    "        o = self.sigmoid(self.z7  + b[0]) # final activation function\n",
    "        return o\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "    # activation function\n",
    "        return 1/(1+np.exp(-s))\n",
    "\n",
    "NN = Neural_Network()\n",
    "\n",
    "out = NN.forward(x)\n",
    "\n",
    "ce = -np.dot(y, np.log(out))\n",
    "\n",
    "print(\"Predicted Output: \\n\", str(out))\n",
    "print(\"Actual Output: \\n\", str(y))\n",
    "print(\"Cross-Entropy: \\n\", str(ce))\n",
    "print(out[2])\n",
    "print(-np.dot(y, np.log(out[2])))\n",
    "print(-np.mean(p*np.log2(q[2]) + (1-p)*np.log2(1-q[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 dense layers with sigmoid (each with a 10 dimensional hidden layer output) + an affine with sigmoid output.\n",
    "\n",
    " \n",
    "\n",
    "So, to be extra clear, your code should EITHER read something like this (#affines before the logistic head == # layers):\n",
    "\n",
    " \n",
    "\n",
    "h1 = ...\n",
    "\n",
    "h2 = ...\n",
    "\n",
    "h3 = ...\n",
    "\n",
    "z_final = ...\n",
    "\n",
    "p = ...\n",
    "\n",
    "xe_loss = ...\n",
    "\n",
    " \n",
    "\n",
    "OR something like this (arguing # affines == # layers):\n",
    "\n",
    " \n",
    "\n",
    "h1 = ...\n",
    "\n",
    "h2 = ...\n",
    "\n",
    "z_final = ...\n",
    "\n",
    "p = ...\n",
    "\n",
    "xe_loss = ...\n",
    "\n",
    " \n",
    "\n",
    "We'll accept either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Answer Questions\n",
    "\n",
    "1.  What is the probability of the third example?\n",
    "2.  What is the cross-entropy loss if its label is negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E: Softmax\n",
    "\n",
    "Recall that softmax(z) is a vector with the same length as z, and whose components are:  $softmax(z)_i = \\frac{e^{z_i}}{\\Sigma_j e^{z_j}}$.\n",
    "\n",
    "### Short Answer Questions\n",
    "\n",
    "1. If the logits coming from the main body of the network are [1, 2, 3], what is the probability of the middle class?\n",
    "2. What is the cross-entropy loss if the correct class is the last one? (i.e. corresponding to logit=3)?\n",
    "3. If you had such a three-class classification problem, what would the dimensions of W and b be for the last layer of the feed forward neural network above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:  [ 2.71828183  7.3890561  20.08553692]\n",
      "output sum:  30.19287485057736\n",
      "model probability (q):  [0.09003057 0.24472847 0.66524096]\n",
      "probability of middle class:  0.24472847105479767\n",
      "CE:  0.4076059644443803\n",
      "CE2:  0.5880511035406706\n"
     ]
    }
   ],
   "source": [
    "logits = np.array([1,2,3])\n",
    "exp_l = np.exp(logits)\n",
    "print('outputs: ', exp_l)\n",
    "\n",
    "sum_exp_l = np.sum(exp_l)\n",
    "print('output sum: ', sum_exp_l)\n",
    "\n",
    "q = exp_l/sum_exp_l\n",
    "print('model probability (q): ', q)\n",
    "print('probability of middle class: ', q[1])\n",
    "\n",
    "p = 1\n",
    "ce = -np.dot(p, np.log(q[2]))\n",
    "print('CE: ', ce)  # is this LOSS? or just CEb\n",
    "\n",
    "\n",
    "p = 1\n",
    "ce2 = -np.dot(p, np.log2(q[2]))\n",
    "print('CE2: ', ce2)  # is this LOSS? or just CEb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cocoxu.github.io/courses/5525_slides_spring17/06_perceptron.pdf\n",
    "# In practice, can represent this with one giant weight vector and repeated features for each category (10, 15 (3*5)) maybe\n",
    "# hidden weights are (10,15) - final weight is (10,3) - b is (3,)\n",
    "# B3 is not not last layer so it could be 10,15 and b3 is (3,)\n",
    "\n",
    "w3 = np.ones((10,3))\n",
    "# z3 = np.ones((10,15))#?\n",
    "print(w3)\n",
    "x = np.array([1,2,3])\n",
    "b = np.array([-1,-2,-3])\n",
    "#rint(h)\n",
    "# print(np.dot(W_out, z3))#?\n",
    "print(w3-b)\n",
    "print(b.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
